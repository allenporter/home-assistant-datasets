# serializer version: 1
# name: test_dataset_cards
  list([
    DatasetCard(name='assist', description='A dataset built to exercise the Home Assistant LLM API. The homes for this\ndataset were synthetically generated using gpt-3.5, and then manually curated\nto exercise the Home Assistant intents for controlling devices. The sentences\nwere made intentionally more difficult than the existing assistant NLP for\nshowcasing larger model reasoning capabilities.', urls=['https://github.com/allenporter/home-assistant-datasets/tree/main/datasets/assist', 'https://developers.home-assistant.io/blog/2024/05/20/llm-api/'], count=5, version='v2', path=None, config_entry_data=None, config_entry_options=None),
    DatasetCard(name='assist-mini', description='A dataset built to exercise the Home Assistant LLM API. The homes for this\ndataset were synthetically generated using gpt-3.5, and then simplified for\nexercising smaller LLMs. The use cases are not intended to be very tricky or\ncomplicated and aimed at a smaller context window. The number of devices/entities\nin each test is intentionally small (e.g. typically under 5 entities per test) to focus\non tool calling capabilities rather than context retrieval.', urls=['https://github.com/allenporter/home-assistant-datasets/tree/main/datasets/assist-mini'], count=4, version='v1', path=None, config_entry_data=None, config_entry_options=None),
    DatasetCard(name='automations', description='A dataset for evaluating automation generation. The homes for this dataset were\nsynthetically generated using gpt-3.5. This dataset is in development and contains\njust a few initial examples. Each benchmark creates a synthetic home fixture\nand configures the entities with a particular state, then asks for an automation\nfor a specific set of devices.\n\nThe benchmark loads a synthetic home and runs pytest with Home Assistant to\nrun through scenarios that should trigger the automation. It also gives points\nfor getting inputs correct and each problem benchmark exercises different\nscenarios that add to the overall score. The various scenarios are not weighted.', urls=['https://github.com/allenporter/home-assistant-datasets/tree/main/datasets/automations'], count=None, version='v0', path=None, config_entry_data=None, config_entry_options={'llm_hass_api': '', 'max_tokens': 4096}),
    DatasetCard(name='intents', description='A dataset built form the Home Assitant intents repo, modeled after existing\nNLP test cases for the assistant pipeline. This is mean to reuse the\ntests that already exist for the NLP, which turns out to expose some\nweaknesses or differences of interpretation of tasks. It also is a very large\nhome which is challenging for smaller models given the ~100 or so devices. Lastly,\nthere are some tests that have subtle mismatches that are reasonable (e.g.\n"minimium brightness" tests)', urls=['https://github.com/allenporter/home-assistant-datasets/tree/main/datasets/intents', 'https://github.com/home-assistant/intents'], count=None, version=None, path=None, config_entry_data=None, config_entry_options=None),
    DatasetCard(name='questions', description='A dataset built to exercise question and answering capabilities of the Home\nAssistant LLM API. The homes for this dataset were synthetically generated\nand then manually curated to exercise the Home Assistant\nintents for querying device state.\n\nThis dataset is currently in development and is not yet complete. It may\ncontain bugs or incomplete data. We welcome contributions to improve the\ndataset. Please see repo docs for more information on how to contribute.', urls=['https://github.com/allenporter/home-assistant-datasets/tree/main/datasets/questions'], count=10, version=None, path=None, config_entry_data=None, config_entry_options=None),
  ])
# ---
# name: test_models
  list([
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/assist-llm',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'Assist LLM fine tuned on Home Assistant Assist intents based on Llama 3.1 (8B)',
      'domain': 'ollama',
      'model_id': 'assist-llm',
      'rpm': None,
      'urls': list([
        'https://ollama.com/allenporter/assist-llm',
        'https://huggingface.co/allenporter/assist-llm',
        'https://huggingface.co/allenporter/assist-llm-GGUF',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-haiku-20240307',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3 Haiku',
      'domain': 'anthropic',
      'model_id': 'claude-3-haiku',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-haiku',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://functionary.functionary:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'functionary-small-v2.5',
        'llm_hass_api': 'assist',
      }),
      'description': 'A custom open AI integration using functionary small v2.5 (8B) with a modified pre-release llama cpp python server.',
      'domain': 'vicuna_conversation',
      'model_id': 'functionary-small-v2.5',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meetkai/functionary-small-v2.5',
        'https://github.com/abetlen/llama-cpp-python',
        'https://github.com/allenporter/functionary-server',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-pro',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini pro (v1)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-pro',
      'rpm': 30,
      'urls': list([
        'https://deepmind.google/technologies/gemini/pro/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'gemma:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': None,
      'description': 'Gemma (7B) from Google using Ollama',
      'domain': 'ollama',
      'model_id': 'gemma',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/gemma',
        'https://ai.google.dev/gemma/docs/model_card',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4o',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-4o',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4o',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4o',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'host': 'ollama.ollama',
        'huggingface_model': 'fixt/home-3b-v3:latest',
        'model_backend': 'ollama',
        'port': '11434',
        'ssl': False,
      }),
      'config_entry_options': dict({
        'context_length': 2048.0,
        'extra_attributes_to_expose': list([
          'rgb_color',
          'brightness',
          'temperature',
          'humidity',
          'fan_mode',
          'media_title',
          'volume_level',
          'item',
          'wind_speed',
        ]),
        'in_context_examples': False,
        'in_context_examples_file': 'in_context_examples.csv',
        'llm_hass_api': 'home-llm-service-api',
        'max_new_tokens': 128,
        'num_in_context_examples': 4.0,
        'ollama_json_mode': False,
        'ollama_keep_alive': 30.0,
        'prompt': '''
          You are 'Al', a helpful AI Assistant that controls the devices in a house. Complete the following task as instructed with the information provided only.
          The current time and date is {{ (as_timestamp(now()) | timestamp_custom("%I:%M %p on %A %B %d, %Y", "")) }}
          Services: {{ formatted_tools }}
          Devices:
          {{ formatted_devices }}
        ''',
        'prompt_template': 'zephyr',
        'refresh_prompt_per_turn': True,
        'remember_conversation': True,
        'remember_num_interactions': 5,
        'remote_use_chat_endpoint': False,
        'request_timeout': 90.0,
        'service_call_regex': '```homeassistant\\n([\\S \\t\\n]*?)```',
        'temperature': 0.1,
        'tool_format': 'min_tool_format',
        'tool_multi_turn_chat': False,
        'top_k': 40.0,
        'top_p': 1.0,
        'typical_p': 1.0,
      }),
      'description': 'The home-llm v3 model based on Phi (3B) and custom component using service calls to control Home Assistant.',
      'domain': 'llama_conversation',
      'model_id': 'home-llm',
      'rpm': None,
      'urls': list([
        'https://github.com/acon96/home-llm/',
        'https://huggingface.co/acon96/Home-3B-v3-GGUF',
        'https://ollama.com/fixt/home-3b-v3',
      ]),
      'version': 2,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'llama3-groq-tool-use:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Groq tool use model fine tuned from llama3 (8B) using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3-groq-tool-use',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3-groq-tool-use',
        'https://console.groq.com/docs/tool-use',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'llama3:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': None,
      'description': 'Llama 3 (8B) from Meta using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'mistral:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Mistral V3 (7B) using Ollama',
      'domain': 'ollama',
      'model_id': 'mistral-v3',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3',
        'https://ollama.com/library/mistral',
        'https://mistral.ai/news/announcing-mistral-7b/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/xlam:1b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'XLam (1B) model from Salesforce using Ollama',
      'domain': 'ollama',
      'model_id': 'xlam-1b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/Salesforce/xLAM-1b-fc-r',
        'https://github.com/SalesforceAIResearch/xLAM',
        'https://ollama.com/allenporter/xlam:1b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/xlam:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 4096,
      }),
      'description': 'XLam (7B) model from Salesforce using Ollama with 4k context window.',
      'domain': 'ollama',
      'model_id': 'xlam-7b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/Salesforce/xLAM-7b-fc-r',
        'https://github.com/SalesforceAIResearch/xLAM',
        'https://ollama.com/allenporter/xlam:7b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': None,
      'config_entry_options': None,
      'description': 'The Home Assisatnt NLP assistant pipeline',
      'domain': 'homeassistant',
      'model_id': 'assistant',
      'rpm': None,
      'urls': list([
        'https://github.com/home-assistant/hassil',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-haiku-20241022',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3.5 Haiku',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-haiku',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/3-5-models-and-computer-use',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-sonnet-20240620',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3.5 Sonnet',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-sonnet',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-5-sonnet',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-1.5-flash-latest',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash (v1.5)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-1.5-flash',
      'rpm': 100,
      'urls': list([
        'https://blog.google/products/gemini/google-gemini-new-features-july-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash-lite',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash lite (v2.0) (exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash-lite',
      'rpm': 2000,
      'urls': list([
        'https://developers.googleblog.com/en/start-building-with-the-gemini-2-0-flash-family/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash (v2.0)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash',
      'rpm': 1000,
      'urls': list([
        'https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-pro-exp-02-05',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini Pro (v2.0) (Exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.5-pro-preview-03-25',
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Google Generative AI integration using Gemini 2.5, a thinking model, designed
        to tackle increasingly complex problems.
      ''',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.5-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:27b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-27b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:4b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-4b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-3.5-turbo-0125',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-3.5 (175B)',
      'domain': 'openai_conversation',
      'model_id': 'gpt-3.5',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-3-5-turbo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4o-mini',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-4o-mini',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4o-mini',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4o-mini',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.1:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'Llama 3.1 (8B) from Meta using Ollama with 8k t window.',
      'domain': 'ollama',
      'model_id': 'llama3.1',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct',
        'https://ollama.com/library/llama3.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:1b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-1b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-1B',
        'https://ollama.com/library/llama3.2:1b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:3b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-3b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-3B',
        'https://ollama.com/library/llama3.2:3b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://openai_api_replica:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'llama-3.3-70b',
        'llm_hass_api': 'assist',
      }),
      'description': 'Llama 3.3 AWQ running in an OpenAI API-compatible server',
      'domain': 'vicuna_conversation',
      'model_id': 'llama3.3-awq',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/casperhansen/llama-3.3-70b-instruct-awq',
        'https://huggingface.co/docs/transformers/en/quantization/awq',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.3:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Llama 3.3 (70B) from Meta using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3.3',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3.3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'mistral-nemo',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
      'domain': 'ollama',
      'model_id': 'mistral-nemo',
      'rpm': None,
      'urls': list([
        'https://mistral.ai/news/mistral-nemo/',
        'https://ollama.com/library/mistral-nemo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:14b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-14b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-32b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwq:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'QwQ is the reasoning model of the Qwen series.',
      'domain': 'ollama',
      'model_id': 'qwq',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwq-32b/',
        'https://ollama.com/library/qwq',
      ]),
      'version': None,
    }),
  ])
# ---
