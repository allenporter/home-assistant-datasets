# serializer version: 1
# name: test_models
  list([
    dict({
      'categories': list([
      ]),
      'config_entry_data': None,
      'config_entry_options': None,
      'cost': None,
      'description': 'The Home Assisatnt NLP assistant pipeline',
      'domain': 'homeassistant',
      'model_id': 'assistant',
      'rpm': None,
      'urls': list([
        'https://github.com/home-assistant/hassil',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'anthropic',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-haiku-20241022',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 0.8,
        'notes': None,
        'output_tokens': 4.0,
      }),
      'description': '''
        Anthropic integration using Claude 3.5 Haiku, a model with intelligence at
        blazing speeds
  
      ''',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-haiku',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/3-5-models-and-computer-use',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'anthropic',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-sonnet-20240620',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 3.0,
        'notes': None,
        'output_tokens': 15.0,
      }),
      'description': 'Anthropic integration using Claude 3.5 Sonnet',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-sonnet',
      'rpm': 20,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-5-sonnet',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'anthropic',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-7-sonnet-20250219',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 3.0,
        'notes': None,
        'output_tokens': 15.0,
      }),
      'description': '''
        Anthropic integration using Claude 3.7 Sonnet, high level of intelligence and
        capability with toggleable extended thinking
  
      ''',
      'domain': 'anthropic',
      'model_id': 'claude-3-7-sonnet',
      'rpm': 20,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-7-sonnet',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-1.5-flash-latest',
        'llm_hass_api': 'assist',
        'max_tokens': 500,
      }),
      'cost': dict({
        'input_tokens': 0.08,
        'notes': 'Free tier is available',
        'output_tokens': 0.3,
      }),
      'description': 'Google Generative AI integration using gemini flash (v1.5)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-1.5-flash',
      'rpm': 100,
      'urls': list([
        'https://blog.google/products/gemini/google-gemini-new-features-july-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash-lite',
        'llm_hass_api': 'assist',
        'max_tokens': 500,
      }),
      'cost': dict({
        'input_tokens': 0.08,
        'notes': 'Free tier is available',
        'output_tokens': 0.3,
      }),
      'description': 'Google Generative AI integration using gemini flash lite (v2.0) (exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash-lite',
      'rpm': 2000,
      'urls': list([
        'https://developers.googleblog.com/en/start-building-with-the-gemini-2-0-flash-family/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash',
        'llm_hass_api': 'assist',
        'max_tokens': 500,
      }),
      'cost': dict({
        'input_tokens': 0.1,
        'notes': 'Free tier is available',
        'output_tokens': 0.4,
      }),
      'description': 'Google Generative AI integration using gemini flash (v2.0)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash',
      'rpm': 1000,
      'urls': list([
        'https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-pro-exp-02-05',
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': 'Google Generative AI integration using gemini Pro (v2.0) (Exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.5-flash-preview-04-17',
        'llm_hass_api': 'assist',
        'max_tokens': 1500,
      }),
      'cost': dict({
        'input_tokens': 0.15,
        'notes': 'Free tier is available',
        'output_tokens': 0.6,
      }),
      'description': 'Google Generative AI integration using gemini flash (v2.5)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.5-flash',
      'rpm': 500,
      'urls': list([
        'https://developers.googleblog.com/en/start-building-with-gemini-25-flash/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.5-pro-preview-03-25',
        'llm_hass_api': 'assist',
        'max_tokens': 1500,
      }),
      'cost': dict({
        'input_tokens': 1.25,
        'notes': 'Free tier is available',
        'output_tokens': 10.0,
      }),
      'description': '''
        Google Generative AI integration using Gemini 2.5, a thinking model, designed
        to tackle increasingly complex problems.
      ''',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.5-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:12b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-12b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:27b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-27b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'google',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:4b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-4b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-3.5-turbo-0125',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 0.5,
        'notes': None,
        'output_tokens': 1.5,
      }),
      'description': 'Open AI Conversation integration using gpt-3.5 (175B)',
      'domain': 'openai_conversation',
      'model_id': 'gpt-3.5',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-3-5-turbo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4.1-mini',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 0.4,
        'notes': None,
        'output_tokens': 1.6,
      }),
      'description': '''
        Open AI Conversation integration using gpt-4.1-mini. This model is
        balanced for intelligence, speed, and cost.
  
      ''',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4.1-mini',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4.1-mini',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4.1-nano',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 0.1,
        'notes': None,
        'output_tokens': 0.4,
      }),
      'description': '''
        Open AI Conversation integration using gpt-4.1-minanoni. This model is the
        fastest, most cost-effective GPT-4.1 model
  
      ''',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4.1-nano',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4.1-nano',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4.1',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 2.0,
        'notes': None,
        'output_tokens': 8.0,
      }),
      'description': '''
        Open AI Conversation integration using gpt-4.1. This is a
        flagship GPT model for complex tasks
  
      ''',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4.1',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4o-mini',
        'llm_hass_api': 'assist',
      }),
      'cost': dict({
        'input_tokens': 0.15,
        'notes': None,
        'output_tokens': 0.6,
      }),
      'description': 'Open AI Conversation integration using gpt-4o-mini',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4o-mini',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4o-mini',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'openai',
        'cloud',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-o4-mini',
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': '''
        Open AI Conversation integration using gpt-4o-mini. This is
        a faster, more affordable reasoning model.
  
      ''',
      'domain': 'openai_conversation',
      'model_id': 'gpt-o4-mini',
      'rpm': 500,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-o4-mini',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'ibm',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'granite3.3:2b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': '''
        IBM Granite 2B and 8B models are 128K context length language models that
        have been fine-tuned for improved reasoning and instruction-following capabilities.
  
      ''',
      'domain': 'ollama',
      'model_id': 'granite3.3-2b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/ibm-granite/granite-3.3-8b-instruct',
        'https://ollama.com/library/granite3.3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'ibm',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'granite3.3:8b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': '''
        IBM Granite 2B and 8B models are 128K context length language models that
        have been fine-tuned for improved reasoning and instruction-following capabilities.
  
      ''',
      'domain': 'ollama',
      'model_id': 'granite3.3-8b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/ibm-granite/granite-3.3-8b-instruct',
        'https://ollama.com/library/granite3.3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'meta',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'llama3.1:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': 'Llama 3.1 (8B) from Meta using Ollama with 8k t window.',
      'domain': 'ollama',
      'model_id': 'llama3.1',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct',
        'https://ollama.com/library/llama3.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'meta',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:1b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-1b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-1B',
        'https://ollama.com/library/llama3.2:1b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'meta',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:3b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-3b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-3B',
        'https://ollama.com/library/llama3.2:3b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'meta',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'llama3.3:70b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': 'Llama 3.3 (70B) from Meta using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3.3-70b',
      'rpm': None,
      'urls': list([
        'https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/',
        'https://ollama.com/library/llama3.3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'meta',
        'local',
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://openai_api_replica:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'llama-3.3-70b',
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': 'Llama 3.3 AWQ running in an OpenAI API-compatible server',
      'domain': 'vicuna_conversation',
      'model_id': 'llama3.3-awq',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/casperhansen/llama-3.3-70b-instruct-awq',
        'https://huggingface.co/docs/transformers/en/quantization/awq',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'mistral',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'mistral-nemo',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
      'domain': 'ollama',
      'model_id': 'mistral-nemo',
      'rpm': None,
      'urls': list([
        'https://mistral.ai/news/mistral-nemo/',
        'https://ollama.com/library/mistral-nemo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'mistral',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'mistral-small3.1:24b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': 'Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.',
      'domain': 'ollama',
      'model_id': 'mistral-small3.1',
      'rpm': None,
      'urls': list([
        'https://mistral.ai/news/mistral-small-3-1',
        'https://ollama.com/library/mistral-small3.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'microsoft',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'phi4-mini:3.8b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': 'Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.',
      'domain': 'ollama',
      'model_id': 'phi4-mini',
      'rpm': None,
      'urls': list([
        'https://mistral.ai/news/mistral-small-3-1',
        'https://ollama.com/library/mistral-small3.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:0.5b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-0.5b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:14b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-14b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-32b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:3b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-3b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-70b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-7b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://openai_api_replica:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'qwen3-30b-a3b',
        'llm_hass_api': 'assist',
      }),
      'cost': None,
      'description': 'Qwen3 30B (A3B) running in an OpenAI API-compatible server',
      'domain': 'vicuna_conversation',
      'model_id': 'qwen3-30b-gptq',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/Qwen/Qwen3-30B-A3B-GPTQ-Int4',
        'https://huggingface.co/docs/transformers/en/quantization/gptq',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'alibaba',
        'local',
      ]),
      'config_entry_data': dict({
        'model': 'qwq:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'cost': None,
      'description': 'QwQ is the reasoning model of the Qwen series.',
      'domain': 'ollama',
      'model_id': 'qwq-32b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwq-32b/',
        'https://ollama.com/library/qwq',
      ]),
      'version': None,
    }),
  ])
# ---
