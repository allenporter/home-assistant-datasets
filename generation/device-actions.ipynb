{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Actions\n",
    "\n",
    "Generates datasets for performing actions on devices in a synthetic home. This will generate a list\n",
    "of text / voice commands that you can perform in a home. These are not labeled with the outcome\n",
    "which is generated in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import google.generativeai as genai\n",
    "\n",
    "from home_assistant_datasets import secrets\n",
    "from home_assistant_datasets.secrets import get_secret\n",
    "from home_assistant_datasets import model_client\n",
    "\n",
    "secrets.DEFAULT_SECRETS_FILE = \"../secrets.yaml\"\n",
    "\n",
    "# MODEL_ID = \"gpt-3.5-turbo-0125\"\n",
    "# openai = openai.OpenAI(api_key=secrets.get_secret(\"openai_api_key\"))\n",
    "# model = model_client.ModelClient(openai, MODEL_ID)\n",
    "\n",
    "# Gemini flash is higher quality and cheaper model than the GPT alternatives.\n",
    "MODEL_ID = \"gemini-1.5-flash\"\n",
    "genai.configure(api_key=secrets.get_secret(\"google_api_key\"))\n",
    "model = model_client.GoogleClient(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate few-shot exapmles\n",
    "\n",
    "Read the seed data used as a few-shot exampe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "home: mountain-cabin-us\n",
      "device:\n",
      "  name: Kitchen Overhead Light\n",
      "  area: Kitchen\n",
      "  device_type: light\n",
      "  device_info:\n",
      "    model: Smart LED Bulb\n",
      "    manufacturer: Philips\n",
      "    sw_version: 1.2.3\n",
      "capabilities:\n",
      "- Turn on\n",
      "- Turn off\n",
      "---\n",
      "actions:\n",
      "- action: Turn on\n",
      "  sentences:\n",
      "  - Please turn on the kitchen overhead light\n",
      "  - Turn on the kitchen light\n",
      "  - Kitchen light on\n",
      "- action: Turn off\n",
      "  sentences:\n",
      "  - Please turn off the kitchen overhead light\n",
      "  - Turn off the kitchen light\n",
      "  - Kitchen light off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "from synthetic_home import device_types\n",
    "\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "DEVICES_DIR = DATASET_DIR / \"devices-v3\"\n",
    "SEEDS_DIR = pathlib.Path(\"./seeds\")\n",
    "SEED_DEVICE_ACTIONS_FILE = SEEDS_DIR / \"device-actions.yaml\"\n",
    "SEED_DEVICE_ACTIONS_CAPABILITIES_FILE = SEEDS_DIR / \"device-actions-capabilities.yaml\"\n",
    "\n",
    "with open(SEED_DEVICE_ACTIONS_FILE) as f:\n",
    "    seed_device_actions = list(yaml.load_all(f.read(), Loader=yaml.Loader))\n",
    "\n",
    "# This is a fixed list of capabilities that any particular synthetic home device type support\n",
    "with open(SEED_DEVICE_ACTIONS_CAPABILITIES_FILE) as f:\n",
    "    capabilities = {\n",
    "        cap[\"device_type\"]: cap[\"actions\"]\n",
    "        for cap in yaml.load(f.read(), Loader=yaml.Loader)\n",
    "    }\n",
    "\n",
    "seed_devices_prompt = \"\".join(yaml.dump(content, sort_keys=False, explicit_start=True) for content in seed_device_actions)\n",
    "print(seed_devices_prompt)\n",
    "\n",
    "registry = device_types.load_device_type_registry()\n",
    "# Find any devices missing explicit action capabilities definitions\n",
    "missing_devices = [\n",
    "    {\"device_type\": dt, \"actions\": []}\n",
    "    for dt in registry.device_types\n",
    "    if dt not in capabilities\n",
    "]\n",
    "if missing_devices:\n",
    "    print(yaml.dump(missing_devices, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = f\"\"\"\n",
    "You are an expert Smart Home agent who can evaluate the performance of a smart\n",
    "home, and perform useful actions on behalf of a user.\n",
    "\n",
    "A device in Home Assistant represents a physical or virtual object, represented\n",
    "by different entities. A device has attributes for its configuration and state,\n",
    "for example a thermostat may have a mode attribute, or target or current temperature\n",
    "attributes.\n",
    "\n",
    "You generate a simple evaluation dataset for home data. The input dataset\n",
    "contains the home, description information like location, areas, and devices.\n",
    "The output data are actions a user may ask to take on a devie.\n",
    "\n",
    "This is the input yaml document and the output actions yaml document:\n",
    "\n",
    "{seed_devices_prompt}\n",
    "\n",
    "Generate a few sentences to control the device. Answer in yaml plain text and do not answer with markdown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 480, 91)\n",
      "exhaust-fan: 11\n",
      "fan-oscilating: 2\n",
      "garage-door: 6\n",
      "heat-pump: 3\n",
      "hvac: 30\n",
      "light: 203\n",
      "light-dimmable: 85\n",
      "smart-blinds: 1\n",
      "smart-lock: 5\n",
      "smart-plug: 34\n",
      "smart-speaker: 49\n",
      "smart-sprinkler: 17\n",
      "smart-tv: 17\n",
      "switch: 8\n",
      "vacuum: 3\n",
      "water-valve: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import slugify\n",
    "\n",
    "homes = []\n",
    "for path in DEVICES_DIR.glob(\"*.yaml\"):\n",
    "    with path.open(\"r\") as f:\n",
    "        content = f.read()\n",
    "    home_id = path.name.split(\".\")[0]  # Strip the .yaml extension\n",
    "    home_data = yaml.load(content, Loader=yaml.Loader)\n",
    "    homes.append((home_id, home_data))\n",
    "\n",
    "tasks = []\n",
    "no_actions = 0\n",
    "task_types = {}\n",
    "for home_id, home in homes:\n",
    "    home_template = {\n",
    "            \"home\": home_id,\n",
    "            \"location\": home[\"location\"],\n",
    "            \"type\": home[\"type\"],\n",
    "    }\n",
    "    for area, devices in home[\"devices\"].items():\n",
    "        for device in devices or []:\n",
    "            device_type = device[\"device_type\"]\n",
    "            if not (device_caps := capabilities.get(device_type)):\n",
    "                # No supported actions\n",
    "                no_actions += 1\n",
    "                continue\n",
    "            task_types[device_type] = task_types.get(device_type, 0) + 1\n",
    "            device_info = {\n",
    "                    **home_template,\n",
    "                    \"device\": {\n",
    "                        **device,\n",
    "                        \"area\": area,\n",
    "                    },\n",
    "                    \"capabilities\": device_caps,\n",
    "            }\n",
    "            tasks.append(device_info)\n",
    "print((len(homes), len(tasks), no_actions))\n",
    "print(yaml.dump(task_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "home: home4-us\n",
      "location: Coastal town in Florida\n",
      "type: Beach house\n",
      "device:\n",
      "  name: Kids Bathroom Light\n",
      "  device_type: light\n",
      "  device_info:\n",
      "    model: Smart LED Bulb\n",
      "    manufacturer: Philips\n",
      "    sw_version: 1.2.3\n",
      "  area: Kids Bathroom\n",
      "capabilities:\n",
      "- Turn on\n",
      "- Turn off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(tasks)\n",
    "print(yaml.dump(tasks[0], sort_keys=False, explicit_start=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 0:  10%|█         | 48/480 [02:13<15:24,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 20, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 1:  14%|█▍        | 69/480 [03:14<19:18,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 2:  21%|██        | 99/480 [04:35<16:23,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 4:  33%|███▎      | 158/480 [07:12<13:14,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 4:  39%|███▉      | 186/480 [08:47<12:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 19, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 6:  45%|████▌     | 217/480 [10:06<11:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 22, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 6:  57%|█████▋    | 272/480 [12:30<07:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while parsing a block mapping\n",
      "  in \"<unicode string>\", line 13, column 3:\n",
      "    - action: Set position\n",
      "      ^\n",
      "expected <block end>, but found '<scalar>'\n",
      "  in \"<unicode string>\", line 17, column 16:\n",
      "      - [position] the shower\n",
      "                   ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 8:  66%|██████▋   | 318/480 [14:27<07:41,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 15, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 9:  80%|████████  | 386/480 [17:46<04:14,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 21, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 9: 100%|██████████| 480/480 [22:00<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import slugify\n",
    "\n",
    "# Total number of records to generate\n",
    "N_DATAPOINTS = -1\n",
    "\n",
    "DEVICE_ACTIONS_OUTPUT_DIR = DATASET_DIR / \"device-actions-v2\"\n",
    "\n",
    "# Wipe existing summaries\n",
    "shutil.rmtree(DEVICE_ACTIONS_OUTPUT_DIR, ignore_errors=True)\n",
    "DEVICE_ACTIONS_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "random.shuffle(tasks)\n",
    "if N_DATAPOINTS > 0 and len(tasks) > N_DATAPOINTS:\n",
    "    tasks = tasks[:N_DATAPOINTS]\n",
    "\n",
    "skipped = 0\n",
    "with tqdm(total=len(tasks)) as pbar:\n",
    "    for task in tasks:\n",
    "        home_id = slugify.slugify(task[\"home\"], separator=\"-\")\n",
    "        task_id = \"_\".join([\n",
    "              slugify.slugify(task[\"device\"][\"area\"], separator=\"-\"),\n",
    "              slugify.slugify(task[\"device\"][\"name\"], separator=\"-\"),\n",
    "        ])\n",
    "        home_dir = DEVICE_ACTIONS_OUTPUT_DIR / home_id\n",
    "        if not home_dir.exists():\n",
    "            home_dir.mkdir()\n",
    "        with open(DEVICE_ACTIONS_OUTPUT_DIR / home_id / f\"{task_id}.yaml\", \"w\") as action_output:\n",
    "            task_yaml = yaml.dump(task, sort_keys=False, explicit_start=True)\n",
    "            response_obj = None\n",
    "            for i in range(3):\n",
    "                response = model.complete(SUMMARY_PROMPT, task_yaml)\n",
    "                try:\n",
    "                    response_obj = yaml.safe_load(response)\n",
    "                except yaml.YAMLError as err:\n",
    "                    print(err)\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            if response_obj is not None:\n",
    "                updated_task = task.copy()\n",
    "                updated_task.update({\"actions\": response_obj})\n",
    "                action_output.write(yaml.dump(updated_task, explicit_start=True, sort_keys=False))\n",
    "            pbar.set_description(f\"Skipped {skipped}\")\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Actions Fixtures\n",
    "\n",
    "Generate test fixtures from the device actions datasets. This will create the\n",
    "home inventory to power the device actions data collections steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 479 1908\n",
      "---\n",
      "light: 619\n",
      "smart-plug: 109\n",
      "light-dimmable: 636\n",
      "hvac: 103\n",
      "exhaust-fan: 33\n",
      "smart-speaker: 141\n",
      "switch: 25\n",
      "smart-sprinkler: 56\n",
      "water-valve: 28\n",
      "garage-door: 18\n",
      "smart-tv: 95\n",
      "heat-pump: 10\n",
      "smart-lock: 15\n",
      "smart-blinds: 3\n",
      "vacuum: 11\n",
      "fan-oscilating: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import pathlib\n",
    "from synthetic_home import synthetic_home\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "DEVICES_DIR = DATASET_DIR / \"devices-v3\"\n",
    "DEVICE_ACTIONS_DIR = DATASET_DIR / \"device-actions-v2\"\n",
    "DEVICE_ACTIONS_FIXTURES_DIR = DATASET_DIR / \"device-actions-v2-fixtures\"\n",
    "\n",
    "shutil.rmtree(DEVICE_ACTIONS_FIXTURES_DIR, ignore_errors=True)\n",
    "DEVICE_ACTIONS_FIXTURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "homes_count = 0\n",
    "devices_count = 0\n",
    "sentences_count = 0\n",
    "device_type_sentences = {}\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DeviceTasks:\n",
    "   device: str\n",
    "   area: str | None\n",
    "   device_id: str | None\n",
    "   entity_id: str | None\n",
    "   sentences: list[str]\n",
    "\n",
    "\n",
    "for devices_file in DEVICES_DIR.glob(\"*.yaml\"):\n",
    "   home_id = devices_file.name.split(\".\")[0]\n",
    "   home = synthetic_home.load_synthetic_home(devices_file)\n",
    "\n",
    "   home_dir = DEVICE_ACTIONS_FIXTURES_DIR / home_id\n",
    "   home_dir.mkdir(exist_ok=True)\n",
    "\n",
    "   inventory = synthetic_home.build_inventory(home)\n",
    "\n",
    "   fixtures = home_dir / \"_fixtures.yaml\"\n",
    "   fixtures.write_text(inventory.to_yaml())\n",
    "\n",
    "   homes_count += 1\n",
    "   category_tasks = {}\n",
    "   for actions_file in (DEVICE_ACTIONS_DIR / home_id).glob(\"*.yaml\"):\n",
    "      devices_count += 1\n",
    "      device_actions = yaml.load(actions_file.read_text(), Loader=yaml.CSafeLoader)\n",
    "      device = device_actions[\"device\"]\n",
    "      category = device[\"device_type\"]\n",
    "      if category not in category_tasks:\n",
    "         category_tasks[category] = []\n",
    "\n",
    "      device_id: str | None = None\n",
    "      for inv_device in inventory.devices:\n",
    "         if inv_device.name.lower() == device[\"name\"].lower():\n",
    "            device_id = inv_device.id\n",
    "            break\n",
    "      assert device_id\n",
    "      entity_id: str | None = None\n",
    "      for inv_entity in inventory.entities:\n",
    "         if inv_entity.name.lower() == device[\"name\"].lower():\n",
    "            if inv_entity.device != device_id:\n",
    "               raise ValueError(f\"Wrong device: {device}\")\n",
    "            entity_id = inv_entity.id\n",
    "            break\n",
    "      assert entity_id\n",
    "      if entity_id.startswith(\"sensor\") or entity_id.startswith(\"binary_sensor\"):\n",
    "         raise ValueError(f\"Matched entity that does not support control {device}\")\n",
    "\n",
    "      actions = device_actions[\"actions\"]\n",
    "      if \"actions\" in actions:\n",
    "         actions = actions[\"actions\"]\n",
    "      for action_data in actions:\n",
    "         sentences = action_data[\"sentences\"]\n",
    "         category_tasks[category].append(\n",
    "            DeviceTasks(device=device[\"name\"], area=device[\"area\"], device_id=device_id, entity_id=entity_id, sentences=sentences)\n",
    "         )\n",
    "      sentences_count += len(sentences)\n",
    "      device_type_sentences[category] = device_type_sentences.get(category, 0) + len(sentences)\n",
    "\n",
    "   for category, tasks in category_tasks.items():\n",
    "      data = {\n",
    "         \"category\": category,\n",
    "         \"tests\": [\n",
    "            dataclasses.asdict(task)\n",
    "            for task in tasks\n",
    "         ]\n",
    "      }\n",
    "      category_file = home_dir / f\"{category}.yaml\"\n",
    "      category_file.write_text(yaml.dump(data, sort_keys=False, explicit_start=True))\n",
    "\n",
    "print(homes_count, devices_count, sentences_count)\n",
    "print(yaml.dump(device_type_sentences, explicit_start=True, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assist pipeline teacher\n",
    "\n",
    "Run the assist pipeline data collection step to generate tool calls against the fixtures\n",
    "\n",
    "```bash\n",
    "$ source venv/bin/activate\n",
    "(venv) $ home-assistant-datasets assist collect --dataset ./datasets/device-actions-v2-fixtures/ --model_output_dir=./datasets/device-actions-v2-collect/ --models=assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save successful Assistant results\n",
    "\n",
    "This saves all the successful results from the assistant pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import slugify\n",
    "import shutil\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "FIXTTURES_DIR = DATASET_DIR / \"device-actions-v2-fixtures\"\n",
    "COLLECT_DIR = DATASET_DIR / \"device-actions-v2-collect\"\n",
    "ASSIST_TEACHER_DIR = COLLECT_DIR / \"assistant\"\n",
    "\n",
    "TRAIN_DIR = COLLECT_DIR / \"train\"\n",
    "shutil.rmtree(TRAIN_DIR, ignore_errors=True)\n",
    "TRAIN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# These can not be used in the training set since they are used for eval\n",
    "EVAL_HOME_IDS = {\n",
    "  \"dom1-pl\",\n",
    "  \"home1-us\",\n",
    "  \"home2-ru\",\n",
    "  \"home5-cn\",\n",
    "  \"home7-dk\",\n",
    "}\n",
    "\n",
    "\n",
    "success = {}\n",
    "total = {}\n",
    "total_sentences = 0\n",
    "\n",
    "for path in FIXTTURES_DIR.glob(\"**/*.yaml\"):\n",
    "    if path.name == \"_fixtures.yaml\":\n",
    "        continue\n",
    "    home_id = path.parent.name\n",
    "    category = path.name.split(\".\")[0]\n",
    "\n",
    "    if home_id in EVAL_HOME_IDS:\n",
    "        continue\n",
    "\n",
    "    fixture_record = yaml.load(path.read_text(), Loader=yaml.CSafeLoader)\n",
    "    matched_tests = []\n",
    "\n",
    "    file_prefix = \"_\".join([\n",
    "        slugify.slugify(home_id, separator=\"_\"),\n",
    "        slugify.slugify(category, separator=\"_\"),\n",
    "    ])\n",
    "    assist_outputs = ASSIST_TEACHER_DIR.glob(f\"{file_prefix}*.yaml\")\n",
    "    for filename in assist_outputs:\n",
    "        total[category] = total.get(category, 0) + 1\n",
    "        record = yaml.load(filename.read_text(), Loader=yaml.Loader)\n",
    "        input_text = record[\"task\"][\"input_text\"]\n",
    "        if record[\"response\"].startswith(\"Sorry\"):\n",
    "            continue\n",
    "        success[category] = success.get(category, 0) + 1\n",
    "\n",
    "        context = record[\"context\"]\n",
    "        conversation_trace = context[\"conversation_trace\"]\n",
    "        if len(conversation_trace) < 2:\n",
    "            continue\n",
    "        if conversation_trace[1][\"event_type\"] != \"tool_call\":\n",
    "            continue\n",
    "        if not (tool_call := conversation_trace[1].get(\"data\")):\n",
    "            continue\n",
    "\n",
    "        for record in fixture_record[\"tests\"]:\n",
    "            sentences = list(record[\"sentences\"])\n",
    "            for sentence in sentences:\n",
    "                if sentence == input_text:\n",
    "                    total_sentences += 1\n",
    "                    matched_tests.append({\n",
    "                        **(record.copy()),\n",
    "                        \"sentences\": [sentence],\n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call[\"intent_name\"],\n",
    "                            \"arguments\": tool_call[\"slots\"],\n",
    "                        }\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "    if not matched_tests:\n",
    "        continue\n",
    "    output_record = fixture_record.copy()\n",
    "    output_record[\"tests\"] = matched_tests\n",
    "    if output_record[\"tests\"]:\n",
    "        (TRAIN_DIR / home_id).mkdir(exist_ok=True)\n",
    "        out_file = TRAIN_DIR / home_id / f\"{category}.yaml\"\n",
    "        out_file.write_text(yaml.dump(output_record, sort_keys=False, explicit_start=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 2332\n",
      "smart-tv - 26 - 13.54% - 1.11%\n",
      "smart-speaker - 26 - 3.83% - 1.11%\n",
      "light-dimmable - 784 - 67.64% - 33.62%\n",
      "water-valve - 13 - 20.31% - 0.56%\n",
      "smart-plug - 140 - 64.22% - 6.00%\n",
      "light - 1824 - 77.78% - 78.22%\n",
      "exhaust-fan - 66 - 100.00% - 2.83%\n",
      "switch - 28 - 63.64% - 1.20%\n",
      "smart-lock - 16 - 53.33% - 0.69%\n",
      "fan-oscilating - 11 - 91.67% - 0.47%\n",
      "smart-sprinkler - 20 - 17.86% - 0.86%\n",
      "hvac - 100 - 33.33% - 4.29%\n",
      "garage-door - 0 - 0.00% - 0.00%\n",
      "vacuum - 2 - 9.09% - 0.09%\n",
      "heat-pump - 8 - 42.11% - 0.34%\n",
      "smart-blinds - 4 - 66.67% - 0.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sentences: {total_sentences}\")\n",
    "for category in total:\n",
    "    s = success.get(category, 0)\n",
    "    t = total[category]\n",
    "    print(f\"{category} - {s} - {100*(s / t):0.2f}% - {100*(s / total_sentences):0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud LLM Teacher\n",
    "\n",
    "Scrape cloud responses with:\n",
    "\n",
    "```\n",
    "$ home-assistant-datasets assist collect --dataset ./datasets/device-actions-v2-fixtures/ --model_output_dir=./datasets/device-actions-v2-collect/ --models=gemini-1.5-flash\n",
    "```\n",
    "\n",
    "Then convert into lower level system messages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import shutil\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "COLLECT_DIR = DATASET_DIR / \"device-actions-v2-collect\"\n",
    "TEACHER_MODEL_DIR = COLLECT_DIR / \"gemini-1.5-flash\"\n",
    "TRAIN_DIR = COLLECT_DIR / \"train\"\n",
    "\n",
    "shutil.rmtree(TRAIN_DIR, ignore_errors=True)\n",
    "TRAIN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for filename in TEACHER_MODEL_DIR.glob(\"*.yaml\"):\n",
    "    record = yaml.load(filename.read_text(), Loader=yaml.Loader)\n",
    "    conversation_trace = record[\"context\"][\"conversation_trace\"]\n",
    "\n",
    "    input_detail = next(filter(lambda x: x[\"event_type\"] == \"async_process\", conversation_trace), None)\n",
    "    input_text = input_detail[\"data\"][\"text\"]\n",
    "\n",
    "    agent_detail = next(filter(lambda x: x[\"event_type\"] == \"agent_detail\", conversation_trace), None)\n",
    "    prompt = agent_detail[\"data\"][\"prompt\"]\n",
    "    prompt = \"\\n\".join(prompt.split(\"\\n\")[1:])  # Strip \"Current time is...\"\n",
    "\n",
    "\n",
    "    tool_call_trace = next(filter(lambda x: x[\"event_type\"] == \"tool_call\", conversation_trace), None)\n",
    "    tool_calls: dict[str, str] | None = None\n",
    "    content: str | None = None\n",
    "    if tool_call_trace:\n",
    "        tool_calls = [{\n",
    "            \"name\": tool_call_trace[\"data\"][\"tool_name\"],\n",
    "            \"arguments\": tool_call_trace[\"data\"][\"tool_args\"],\n",
    "        }]\n",
    "    else:\n",
    "        content = record[\"response\"]\n",
    "    message = {\n",
    "        \"instructions\": prompt,\n",
    "        \"tools\": agent_detail[\"data\"][\"tools\"],\n",
    "        \"input\": input_text,\n",
    "        \"output\": content,\n",
    "        \"tool_calls\": tool_calls,\n",
    "    }\n",
    "    output_file = TRAIN_DIR / filename.name\n",
    "    output_file.write_text(yaml.dump(message, explicit_start=True, sort_keys=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
